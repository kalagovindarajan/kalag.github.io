# KALA GOVINDARAJAN
## DATA CLOUD ARCHITECT


A Cloud Data Architect experienced in designing and developing robust and flexible data architectures for both Cloud and on-premise applications. I carry 15+ years of software engineering experience working on multiple lifecycles of data science, data analytics, data migration, data integration implementations where I have helped Fortune 500 organizations identify key business drivers and challenges through requirements elicitation, advocate effective solutions, develop data strategies to create modern data architectures aligning with business goals. My areas of interest is Machine Learning and Artificial Intelligence systems and help organizations tap the potential from their data using these technologies. 

**EDUCATION**

Masters of Science in Computer Science (Online), The University of Texas at Austin
Bachelor of Computer Science Engineering 


**PROFESSIONAL EXPERIENCE**

 **Snowflake (Jan 2020 to Present)**
 
  a. Provide guidance, proven practices, enterprise frameworks and templates and assist with building the end to end Machine Learning based applications .
  b. Contributed to Data Science Tech Group by working on use-cases like Image Recognition and Analysis using AWS Rekognition for unstructured data stored on Snowflake, DICOM Image Parsing, Computer Vision applications, Image File reader and Anomaly detection.
  QuickStart Materials.
  c. Additionally, served several contributions as a SME towards Item writing for all the certifications, Data science forums, QuickStart and Best practices team, SKE Champion and many other
  initiatives in the org.
 
 **Orange (July 2020 - Dec 2020)**
  
  a. Defined, designed, and developed an implementation proposal for Smart IoT Monitoring solution for
  leading Life Sciences Client on Azure Event Hubs, Stream Analytics, Azure Data Lake, Azure ML and
  Power BI.
  b. Developed multiple POC solutions and created compelling demonstrations on IoT data.
  c. Defined target state and provided thought leadership on cloud infrastructure and cloud services to
  meet the strategic and operational objectives for a leading customer.
 
 **Business & Decision (Jan 2018 - Jul 2020)**
 
  a) Migration of on-premise Data Warehouse on Oracle to Google Cloud platform. An iterative approach
  taken to handle Data warehouse modernization for data science and analytics use-cases by migrating one
  data mart at a time for the pilot implementation. Rebuilt by optimizing the data model and consumption
  layer for Google Big Query. The different phases were.
  1. Re-planning of existing EDW data model involved schema design considerations, de-normalization,
  partitioning and methods for loading data into Google Big Query.
  2. Re-design current normalized star schema on Oracle based Data warehouse to target de-
  normalized table datasets on Big Query.
  3.Analyzed current source data ingestion methods and scale in Google Cloud Platform
  supporting both batch and real-time loading.
  4.Built Data Pipelines processes data through a sequence of connected processing step using
  Google Pub/Sub and Cloud Dataflow.
  
  b) Developed a recommendation engine to predict customer’s next purchase and achieve successful
  Targeted Interactions Strategy that reduced customer churn by 30%. This involved the following steps
  that were carried on SAS:
   1. Preparing data from past consumer purchase history, CRM, external datasets, and user profiles
   2. Built Models to represent the prediction problem by creating labels, generate features for the
  labels and find the best model by evaluating prediction performance for train and test sets.
   3. Predict the customer’s next purchase and automate marketing mail to push offer notification.
   4. Monitor and Management of the models on SAS
   5. Presented the recommendation engine prototype at SAS Global Forum at Dallas, Texas in 2019
 
 **DXC, Computer Sciences Corporation (Dec 2014 - Sep 2016)**
 
  1. Designed and developed a predictive maintenance engine for avionics systems on AWS Sagemaker
  that reduced failure by 40%. Simulated datasets were created under different combinations of
  operational conditions and fault modes. Train deep learning an MXNet model that can learn from
  sequential or time series data. The model training was orchestrated by running a jupyter notebook on a
  SageMaker Notebook instance.
 
 **Infosys (Sep 2008 - Dec 2014)**
  a) Created an EDW to serve a Business Intelligence Dashboard which helps business stakeholders in
  determining the Total Cost of Ownership (TCO) of the various CPE (Customer Premise Equipment) that a
  major American Telecommunication provider delivers to its customers.
  b) Created a data migration solution for the Targeted Interactions group of a major American integrated
  retailer which aims at an incremental business performance.
  c) Created a data migration solution for the Targeted Interactions group of a major American integrated
  retailer which aims at an incremental business performance.

**Skills:**
 1. Cloud Data Platform Architecture – Rich practical experience on all the vertical workload with focus on Data Science
 2. Machine Learning in Cloud – End-to-end ML life cycle including data preparation, Model training,
 Feature Engineering, Optimization, Experimentation and deployment on leading platforms like
 Snowflake, AWS SageMaker and SAS.
 3. Architectural advice and thought leadership for Data Architecture, Data Engineering, Data Lake/Warehouse and ML/Data Analytics competencies.
 4. Programming languages : Python and Java


**Contact:**  

Email: kala.govindarajan@utexas.edu
LinkedIn: https://www.linkedin.com/in/kalag/
